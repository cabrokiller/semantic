{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text files \n",
    "path = '../data/TEXTO_CORREGIDO/*.txt'\n",
    "files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list from txt files\n",
    "# NEED TO MAKE PROCESS TO CHECK IF TXT FILE IS WELL CONFIGURED\n",
    "doc_all = []\n",
    "for file in files:\n",
    "    doc = [x.strip() for x in open(file, encoding='utf-8-sig')]\n",
    "    doc.append(re.findall('[A-Z]{3}[0-9]{8}.', file)[0])\n",
    "    doc_all.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas df and clean it\n",
    "data = pd.DataFrame(data = doc_all,\n",
    "                    columns = (\"gen\", \"pos\", \"neg\", \"cam\", \"id\"))\n",
    "\n",
    "cl_data = data.melt(id_vars='id',\n",
    "          value_vars=['gen','pos','neg','cam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud function\n",
    "\n",
    "def wordcloud_draw(data, color = 'black'):\n",
    "    words = ' '.join(data)\n",
    "    wordcloud = WordCloud(stopwords=stopwords_set,\n",
    "                      background_color=color,\n",
    "                      width=2500,\n",
    "                      height=2000\n",
    "                     ).generate(words)\n",
    "    plt.figure(1,figsize=(13, 13))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = cl_data[cl_data['variable'].isin(['pos','gen'])].value\n",
    "neg = cl_data[cl_data['variable'].isin(['neg','cam'])].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-94773f6efac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwordcloud_draw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"white\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwordcloud_draw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"black\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-5e6afc91652c>\u001b[0m in \u001b[0;36mwordcloud_draw\u001b[1;34m(data, color)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwordcloud_draw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'black'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     wordcloud = WordCloud(stopwords=stopwords_set,\n\u001b[0m\u001b[0;32m      6\u001b[0m                       \u001b[0mbackground_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                       \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords_set' is not defined"
     ]
    }
   ],
   "source": [
    "wordcloud_draw(pos, \"white\")\n",
    "wordcloud_draw(neg, \"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: es_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.1.0/es_core_news_sm-2.1.0.tar.gz#egg=es_core_news_sm==2.1.0 in c:\\users\\corsega\\miniconda3\\envs\\ds\\lib\\site-packages (2.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "parser = Spanish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniza con spacy ( tambien se puede obtenet POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "doc = '. '.join(flatten(doc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir si elegir solo algunas POS\n",
    "\n",
    "\n",
    "def lematiza(frase):\n",
    "    '''Deveulve una lista de términos lematizados de una frase,\n",
    "    además quita palabras STOP, que no sean alpha\n",
    "    y las de una sola letra'''\n",
    "    lemmas = []\n",
    "    for token in docs:\n",
    "        if token.is_punct == False and \\\n",
    "            token.is_stop == False and \\\n",
    "            token.pos_ in ['NOUN', 'PRON', 'ADJ']:\n",
    "                lemmas.append(token.lemma_.lower())\n",
    "    return(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = cl_data[cl_data['variable'] == 'pos'].value.tolist()\n",
    "gen = cl_data[cl_data['variable'] == 'gen'].value.tolist()\n",
    "neg = cl_data[cl_data['variable'] == 'neg'].value.tolist()\n",
    "cam = cl_data[cl_data['variable'] == 'cam'].value.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(frase_lst):\n",
    "    '''Devuelve una lista de listas. Realiza el\n",
    "    el procesamiento de spacy y saca las lemmas'''\n",
    "    lemma_list = [lematiza(frase_tag) for\n",
    "                 frase_tag in [nlp(frase) for\n",
    "                              frase in frase_lst]]\n",
    "    return(lemma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using wordnet de nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is spanish available? I think it should be possible to dl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ludwig/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma2('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is part of UDPipe <http://github.com/ufal/udpipe/>.\n",
    "#\n",
    "# Copyright 2016 Institute of Formal and Applied Linguistics, Faculty of\n",
    "# Mathematics and Physics, Charles University in Prague, Czech Republic.\n",
    "#\n",
    "# This Source Code Form is subject to the terms of the Mozilla Public\n",
    "# License, v. 2.0. If a copy of the MPL was not distributed with this\n",
    "# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n",
    "\n",
    "import ufal.udpipe\n",
    "# ufal.udpipe.Model etc. are SWIG-magic and cannot be detected by pylint\n",
    "# pylint: disable=no-member\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, path):\n",
    "        \"\"\"Load given model.\"\"\"\n",
    "        self.model = ufal.udpipe.Model.load(path)\n",
    "        if not self.model:\n",
    "            raise Exception(\"Cannot load UDPipe model from file '%s'\" % path)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize the text and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
    "        tokenizer = self.model.newTokenizer(self.model.DEFAULT)\n",
    "        if not tokenizer:\n",
    "            raise Exception(\"The model does not have a tokenizer\")\n",
    "        return self._read(text, tokenizer)\n",
    "\n",
    "    def read(self, text, in_format):\n",
    "        \"\"\"Load text in the given format (conllu|horizontal|vertical) and return list of ufal.udpipe.Sentence-s.\"\"\"\n",
    "        input_format = ufal.udpipe.InputFormat.newInputFormat(in_format)\n",
    "        if not input_format:\n",
    "            raise Exception(\"Cannot create input format '%s'\" % in_format)\n",
    "        return self._read(text, input_format)\n",
    "\n",
    "    def _read(self, text, input_format):\n",
    "        input_format.setText(text)\n",
    "        error = ufal.udpipe.ProcessingError()\n",
    "        sentences = []\n",
    "\n",
    "        sentence = ufal.udpipe.Sentence()\n",
    "        while input_format.nextSentence(sentence, error):\n",
    "            sentences.append(sentence)\n",
    "            sentence = ufal.udpipe.Sentence()\n",
    "        if error.occurred():\n",
    "            raise Exception(error.message)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        \"\"\"Tag the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
    "        self.model.tag(sentence, self.model.DEFAULT)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        \"\"\"Parse the given ufal.udpipe.Sentence (inplace).\"\"\"\n",
    "        self.model.parse(sentence, self.model.DEFAULT)\n",
    "\n",
    "    def write(self, sentences, out_format):\n",
    "        \"\"\"Write given ufal.udpipe.Sentence-s in the required format (conllu|horizontal|vertical).\"\"\"\n",
    "\n",
    "        output_format = ufal.udpipe.OutputFormat.newOutputFormat(out_format)\n",
    "        output = ''\n",
    "        for sentence in sentences:\n",
    "            output += output_format.writeSentence(sentence)\n",
    "        output += output_format.finishDocument()\n",
    "\n",
    "        return output\n",
    "\n",
    "# Can be used as\n",
    "#  model = Model('english-ud-1.2-160523.udpipe')\n",
    "#  sentences = model.tokenize(\"Hi there. How are you?\")\n",
    "#  for s in sentences:\n",
    "#      model.tag(s)\n",
    "#      model.parse(s)\n",
    "#  conllu = model.write(sentences, \"conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model('../models/spanish-ancora-ud-2.4-190531.udpipe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = model.tokenize(\"Hola que tal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-288dbdf7cdaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "#      model.tag(s)\n",
    "#      model.parse(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic modeling with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemmas = get_lemmas(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = corpora.Dictionary(pos_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus = [dct.doc2bow(doc, allow_update=True) for doc in pos_lemmas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(mycorpus, smartirs='ntc')\n",
    "# com ose usa esto para el futuro??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram models\n",
    "dataset = '. '.join(pos).split()\n",
    "bigram = models.phrases.Phrases(dataset, min_count=3, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, LdaMulticore\n",
    "# Step 4: Train the LDA model\n",
    "lda_model = LdaMulticore(corpus=mycorpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=3,\n",
    "                         passes=10,\n",
    "                         chunksize=1000,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.026*\"casar\" + 0.020*\"ingresar\" + 0.017*\"personar\" + 0.017*\"gente\" + 0.014*\"positivo\" + 0.013*\"enfermero\" + 0.012*\"profesional\" + 0.012*\"paciente\" + 0.011*\"comer\" + 0.011*\"hospital\"'),\n",
       " (1,\n",
       "  '0.018*\"casar\" + 0.012*\"ingresar\" + 0.012*\"gente\" + 0.010*\"enfermero\" + 0.009*\"paciente\" + 0.009*\"personar\" + 0.009*\"hospital\" + 0.008*\"positivo\" + 0.008*\"comer\" + 0.007*\"coser\"'),\n",
       " (2,\n",
       "  '0.017*\"casar\" + 0.014*\"ingresar\" + 0.012*\"hospital\" + 0.012*\"personar\" + 0.012*\"gente\" + 0.010*\"positivo\" + 0.010*\"comer\" + 0.009*\"profesional\" + 0.009*\"paciente\" + 0.008*\"enfermero\"')]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
